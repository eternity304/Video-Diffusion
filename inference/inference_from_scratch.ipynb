{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1343ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22202fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(arg_list=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unconditioned Video Diffusion Inference\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset-path\", type=str, required=True,\n",
    "        help=\"Directory containing input reference videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained-model-name-or-path\", type=str, required=True,\n",
    "        help=\"Path or HF ID where transformer/vae/scheduler are stored.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-path\", type=str, required=True,\n",
    "        help=\"Path to fine‐tuned checkpoint containing transformer state_dict.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\", type=str, required=True,\n",
    "        help=\"Where to write generated videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-config\", type=str, required=True,\n",
    "        help=\"YAML file describing model params (height, width, num_reference, num_target, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=1,\n",
    "        help=\"Batch size per device (usually 1 for inference).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-inference-steps\", type=int, default=50,\n",
    "        help=\"Number of reverse diffusion steps to run.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed-precision\", type=str, default=\"bf16\",\n",
    "        help=\"Whether to run backbone in 'fp16', 'bf16', or 'fp32'.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42,\n",
    "        help=\"Random seed for reproducibility.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\", type=int, default=False,\n",
    "        help=\"Whether to shuffle dataset. Usually False for inference.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--is-uncond\", type=bool, default=False,\n",
    "        help=\"\"\n",
    "    )\n",
    "\n",
    "    # If arg_list is None, argparse picks up sys.argv; \n",
    "    # otherwise it treats arg_list as the full argv list.\n",
    "    return parser.parse_args(arg_list)\n",
    "\n",
    "args = [\n",
    "    \"--dataset-path\", \"/scratch/ondemand28/harryscz/head_audio/data/data256/uv\",\n",
    "    \"--pretrained-model-name-or-path\", \"/scratch/ondemand28/harryscz/model/CogVideoX-2b\",\n",
    "    \"--checkpoint-path\",  \"/scratch/ondemand28/harryscz/head_audio/trainOutput/checkpoint-1000.pt\",\n",
    "    \"--output-dir\",  \"/scratch/ondemand28/harryscz/diffusion/videoOut\",\n",
    "    \"--model-config\",  \"/scratch/ondemand28/harryscz/diffusion/train/model_config.yaml\",\n",
    "    \"--batch-size\",  \"1\",\n",
    "    \"--num-inference-steps\",  \"50\",\n",
    "    \"--mixed-precision\",  \"no\",\n",
    "    \"--seed\",  \"42\",\n",
    "    \"--shuffle\",  \"0\",\n",
    "]\n",
    "\n",
    "args = parse_args(args)\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52927352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1011943/782281243.py\", line 32, in <module>\n",
      "    logger.info(\"Accelerator state:\", accelerator.state)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1806, in info\n",
      "    self.log(INFO, msg, *args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/accelerate/logging.py\", line 63, in log\n",
      "    self.logger.log(level, msg, *args, **kwargs)\n",
      "Message: 'Accelerator state:'\n",
      "Arguments: (Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      ",)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n",
    "if args.mixed_precision.lower() == \"fp16\":\n",
    "    dtype = torch.float16\n",
    "elif args.mixed_precision.lower() == \"bf16\":\n",
    "    dtype = torch.bfloat16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir,\n",
    "                                                    logging_dir=os.path.join(args.output_dir, \"logs\"))\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)\n",
    "accelerator = Accelerator(mixed_precision=args.mixed_precision,\n",
    "                            project_config=accelerator_project_config,\n",
    "                            kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "# 2.4 Set random seed\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed + accelerator.process_index)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(\"Accelerator state:\", accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4addf107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/10/2025 14:37:17 - INFO - __main__ - Number of test examples: 10\n"
     ]
    }
   ],
   "source": [
    "#### Dataset #####\n",
    "# Video data have shape [B, C, F, H, W]\n",
    "\n",
    "from data.VideoDataset import VideoDataset \n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "dataset = VideoDataset(\n",
    "    videos_dir=args.dataset_path,\n",
    "    num_ref_frames=1,\n",
    "    num_target_frames=49\n",
    ")\n",
    "if args.shuffle:\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=accelerator.num_processes,\n",
    "        rank=accelerator.process_index,\n",
    "        shuffle=True\n",
    "    )\n",
    "else:\n",
    "    sampler = None\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    # sampler=sampler,\n",
    "    collate_fn=lambda x: x[0],   # since dataset returns already‐batched items\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "logger.info(f\"Number of test examples: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "550ca31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b were not used when initializing CAPVideoXTransformer3DModel: \n",
      " ['patch_embed.text_proj.weight, patch_embed.text_proj.bias']\n",
      "Some weights of CAPVideoXTransformer3DModel were not initialized from the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b and are newly initialized: ['patch_embed.audio_proj.weight', 'patch_embed.cond_proj.bias', 'patch_embed.ref_temp_proj.bias', 'patch_embed.audio_proj.bias', 'patch_embed.cond_proj.weight', 'patch_embed.ref_temp_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#### Load Model ####\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXDDIMScheduler\n",
    "from model.cap_transformer import CAPVideoXTransformer3DModel\n",
    "\n",
    "transformer = CAPVideoXTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float32,\n",
    "    cond_in_channels=1,  # only one channel (the ref_mask)\n",
    "    sample_width=model_config[\"width\"] // 8,\n",
    "    sample_height=model_config[\"height\"] // 8,\n",
    "    max_text_seq_length=1,\n",
    "    max_n_references=model_config[\"max_n_references\"],\n",
    "    apply_attention_scaling=model_config[\"use_growth_scaling\"],\n",
    "    use_rotary_positional_embeddings=False,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "scheduler = CogVideoXDDIMScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"scheduler\",\n",
    ")\n",
    "\n",
    "vae.eval().to(dtype)\n",
    "transformer.eval().to(dtype)\n",
    "\n",
    "vae, transformer, scheduler, data_loader = accelerator.prepare(vae, transformer, scheduler, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cbbe0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['video_chunks', 'cond_chunks', 'chunk_is_ref', 'raw_audio'])\n",
      "torch.Size([1, 3, 50, 256, 256])\n",
      "dict_keys(['ref_mask'])\n",
      "torch.Size([1, 50, 256, 256, 3])\n",
      "[tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(data_loader):\n",
    "    print(batch.keys())\n",
    "    for chunk_id in range(len(batch[\"video_chunks\"])):\n",
    "        print(batch[\"video_chunks\"][chunk_id].shape)\n",
    "        print(batch[\"cond_chunks\"].keys())  # Mask for each frames over H and W and channel suggesting which one works as a condition\n",
    "                                            # list of tensor masks for cond chunks\n",
    "        print(batch['cond_chunks']['ref_mask'][0].shape)\n",
    "        print(batch[\"chunk_is_ref\"]) # list of length frame of bool saying which on is a condition \n",
    "        print(batch[\"raw_audio\"]) # passed as none\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10caf554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 50/50 [01:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved !\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from inference.inference_pipeline import *\n",
    "\n",
    "pipe = VideoDiffusionPipeline(\n",
    "    vae=vae,\n",
    "    transformer=transformer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "batch = next(iter(data_loader))\n",
    "videos = pipe(batch, num_inference_steps=50)\n",
    "save_video(videos[0][0], \"/scratch/ondemand28/harryscz/diffusion/videoOut/try.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5760fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import retrieve_timesteps\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "\n",
    "def save_video(video : torch.tensor, save_path : str, fps : int = 16):\n",
    "    video_np = video.permute(1, 2, 3, 0).cpu().numpy()  # [49, 256, 256, 3]\n",
    "\n",
    "    video_np = (video_np * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    imageio.mimsave(save_path, video_np, fps=fps)\n",
    "    \n",
    "    print(\"Saved !\")\n",
    "\n",
    "class VideoDiffusionPipeline(DiffusionPipeline):\n",
    "    \"\"\"\n",
    "    A custom diffusion pipeline that mirrors your manual inference loop,\n",
    "    but inherits from DiffusionPipeline to leverage no-grad, mixed-precision,\n",
    "    and buffer reuse for maximum efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae,\n",
    "        transformer,\n",
    "        scheduler,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_modules(vae=vae, transformer=transformer, scheduler=scheduler)\n",
    "\n",
    "        # Scale factors for spatial/temporal axes\n",
    "        self.vae_scale_factor_spatial = 2 ** (len(self.vae.config.block_out_channels) - 1)\n",
    "        self.vae_scale_factor_temporal = getattr(self.vae.config, \"temporal_compression_ratio\", 1)\n",
    "\n",
    "        # Video post-processor\n",
    "        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n",
    "\n",
    "    def decode_latents(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        latents = latents.permute(0, 2, 1, 3, 4)  # [batch_size, num_channels, num_frames, height, width]\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents\n",
    "\n",
    "        frames = self.vae.decode(latents).sample\n",
    "        return frames\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: Dict[str, Union[List[torch.FloatTensor], Dict[str, List[torch.FloatTensor]]]],\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 1.0,\n",
    "        seed: int = 42,\n",
    "        output_type: str = \"pil\",\n",
    "        return_dict: bool = False,\n",
    "        return_latent: bool = False,\n",
    "        return_pil: bool = False,\n",
    "        return_decode_latent : bool = True,\n",
    "    ) -> Union[List, Dict]:\n",
    "        device = self._execution_device\n",
    "        dtype = self.transformer.dtype\n",
    "        generator = torch.Generator(device=device)\n",
    "        generator.manual_seed(seed+1)\n",
    "\n",
    "        # 1) Extract & encode\n",
    "        latent_chunks: List[torch.Tensor] = []\n",
    "        ref_mask_chunks: List[torch.Tensor] = []\n",
    "        sequence_infos: List[tuple] = []\n",
    "\n",
    "        for i, video in enumerate(batch[\"video_chunks\"]):\n",
    "            # video: [B, C, F, H, W]\n",
    "            video = video.to(device=device, dtype=dtype)\n",
    "            # Initialize first frame and set rest as random noise\n",
    "            video[:, :, 1:, :, :] = torch.randn(video[:, :, 1:, :, :].shape, generator=generator, device=device)\n",
    "            with torch.no_grad(): dist = self.vae.encode(video).latent_dist.sample()\n",
    "            latent = dist * self.vae.config.scaling_factor\n",
    "            latent = latent.permute(0, 2, 1, 3, 4).contiguous()  # [B, F, C_z, h, w]\n",
    "            latent_chunks.append(latent)\n",
    "\n",
    "            # mask: batch[\"cond_chunks\"][\"ref_mask\"][i] shape [B, F, H, W, C_mask]\n",
    "            rm = batch[\"cond_chunks\"][\"ref_mask\"][i]\n",
    "            rm = rm.to(device=device, dtype=dtype).permute(0, 4, 1, 2, 3)\n",
    "            # now [B, C_mask, F, H, W]\n",
    "            ref_mask_chunks.append(rm)\n",
    "\n",
    "            # sequence info\n",
    "            is_ref = batch.get(\"chunk_is_ref\", [False] * len(latent_chunks))[i]\n",
    "            seq = torch.arange(0, latent.shape[1], device=device)\n",
    "            sequence_infos.append((is_ref, seq))\n",
    "\n",
    "        # 2) Build 2× for classifier-free guidance\n",
    "        latents = latent_chunks\n",
    "        masks   = [torch.cat([m, torch.zeros_like(m)], dim=0) for m in ref_mask_chunks]\n",
    "        # keep ref_latents for mixing\n",
    "        ref_latents = [torch.cat([z, torch.zeros_like(z)], dim=0) for z in latent_chunks]\n",
    "\n",
    "        # 3) dummy audio/text embeddings (adjust if you have real ones)\n",
    "        B2 = latents[0].shape[0] * 2\n",
    "        total_F = sum(z.shape[1] for z in latents)\n",
    "        audio_embeds = torch.zeros((B2, total_F, 768), dtype=dtype, device=device)\n",
    "        text_embeds  = torch.zeros((B2, 1,\n",
    "            self.transformer.config.attention_head_dim * self.transformer.config.num_attention_heads\n",
    "        ), dtype=dtype, device=device)\n",
    "\n",
    "        # 4) timesteps\n",
    "        timesteps, _ = retrieve_timesteps(self.scheduler, num_inference_steps, device=device)\n",
    "\n",
    "        # 5) optional fuse QKV once\n",
    "        # try:\n",
    "        #     self.transformer.fuse_qkv_projections()\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "\n",
    "        # 6) denoising loop\n",
    "        old_pred_original_samples = [None] * len(latents)\n",
    "        for i, t in enumerate(tqdm(timesteps, desc=\"Inference Progress\")):\n",
    "            latent_model_inputs = [torch.cat([chunks] * 2, dim=0)for chunks in latents]\n",
    "            B2, F, C, H, W = latent_model_inputs[0].shape\n",
    "            # one zero condition tensor\n",
    "            zero_cond = torch.zeros((B2, F, 1, H, W), dtype=dtype, device=device)\n",
    "\n",
    "            # single forward\n",
    "            noise_preds = self.transformer(\n",
    "                hidden_states=latent_model_inputs,\n",
    "                encoder_hidden_states=text_embeds,\n",
    "                audio_embeds=audio_embeds,\n",
    "                condition=[zero_cond] * len(latent_model_inputs),\n",
    "                sequence_infos=[[False, torch.arange(chunk.shape[1])]for chunk in latents],\n",
    "                timestep=t.expand(B2),\n",
    "                image_rotary_emb=None,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # apply guidance, scheduler.step, then mixing\n",
    "            new_latents = []\n",
    "            new_old_pred_original_samples = []\n",
    "\n",
    "            for noise_pred, old_pred_original_sample, latent in zip(noise_preds, old_pred_original_samples, latents):\n",
    "                noise_pred, noise_pred_uncond = noise_pred.chunk(2, dim=0)\n",
    "                # noise_pred = noise_pred_uncond + guidance_scale * (noise_pred - noise_pred_uncond)\n",
    "            \n",
    "                latent, old_pred_original_sample = scheduler.step(\n",
    "                    noise_pred,\n",
    "                    old_pred_original_sample,\n",
    "                    t,\n",
    "                    timesteps[i - 1] if i > 0 else None,\n",
    "                    latent,\n",
    "                    eta=0.0,\n",
    "                    generator=generator\n",
    "                )\n",
    "\n",
    "                new_latents.append(latent)\n",
    "                new_old_pred_original_samples.append(old_pred_original_sample)\n",
    "\n",
    "            latents = new_latents\n",
    "            old_pred_original_samples = list(new_old_pred_original_samples)\n",
    "\n",
    "        # 7) decode to videos\n",
    "        videos = []\n",
    "        if return_latent:\n",
    "            return latent\n",
    "        \n",
    "        if return_decode_latent:\n",
    "            return self.decode_latents(latent)\n",
    "        \n",
    "        for latent in latents:\n",
    "            dec = latent.permute(0, 2, 1, 3, 4) / self.vae.config.scaling_factor\n",
    "            frames = self.vae.decode(dec).sample\n",
    "            video = self.video_processor.postprocess_video(video=frames, output_type=output_type)\n",
    "            videos.append(video)\n",
    "\n",
    "        return {\"frames\": videos} if return_dict else videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14863e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(args.seed)\n",
    "pipe = VideoDiffusionPipeline(vae, transformer, scheduler)\n",
    "pipe = pipe.to(device).to(torch.float32)   \n",
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc150ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 100%|██████████| 50/50 [01:29<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "videos = pipe(batch, num_inference_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bd751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved !\n"
     ]
    }
   ],
   "source": [
    "save_video(videos[0], \"/scratch/ondemand28/harryscz/diffusion/videoOut/try.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
