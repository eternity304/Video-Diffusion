{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1343ccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/horai.dgpsrv/ondemand28/harryscz/diffusion'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import math\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22202fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(arg_list=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unconditioned Video Diffusion Inference\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset-path\", type=str, required=True,\n",
    "        help=\"Directory containing input reference videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained-model-name-or-path\", type=str, required=True,\n",
    "        help=\"Path or HF ID where transformer/vae/scheduler are stored.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-path\", type=str, required=True,\n",
    "        help=\"Path to fine‐tuned checkpoint containing transformer state_dict.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\", type=str, required=True,\n",
    "        help=\"Where to write generated videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-config\", type=str, required=True,\n",
    "        help=\"YAML file describing model params (height, width, num_reference, num_target, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=1,\n",
    "        help=\"Batch size per device (usually 1 for inference).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-inference-steps\", type=int, default=50,\n",
    "        help=\"Number of reverse diffusion steps to run.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed-precision\", type=str, default=\"bf16\",\n",
    "        help=\"Whether to run backbone in 'fp16', 'bf16', or 'fp32'.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42,\n",
    "        help=\"Random seed for reproducibility.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\", type=int, default=False,\n",
    "        help=\"Whether to shuffle dataset. Usually False for inference.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--is-uncond\", type=bool, default=False,\n",
    "        help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample-frames\", type=int, default=50\n",
    "    )\n",
    "\n",
    "    # If arg_list is None, argparse picks up sys.argv; \n",
    "    # otherwise it treats arg_list as the full argv list.\n",
    "    return parser.parse_args(arg_list)\n",
    "\n",
    "args = [\n",
    "    \"--dataset-path\", \"/scratch/ondemand28/harryscz/head_audio/head/data/vfhq-fit\",\n",
    "    \"--pretrained-model-name-or-path\", \"/scratch/ondemand28/harryscz/model/CogVideoX-2b\",\n",
    "    \"--checkpoint-path\",  \"/scratch/ondemand28/harryscz/head_audio/trainOutput/checkpoint-6000.pt\",\n",
    "    \"--output-dir\",  \"/scratch/ondemand28/harryscz/diffusion/videoOut\",\n",
    "    \"--model-config\",  \"/scratch/ondemand28/harryscz/diffusion/train/model_config.yaml\",\n",
    "    \"--batch-size\",  \"1\",\n",
    "    \"--num-inference-steps\",  \"50\",\n",
    "    \"--mixed-precision\",  \"no\",\n",
    "    \"--seed\",  \"42\",\n",
    "    \"--shuffle\",  \"0\",\n",
    "    \"--sample-frames\", \"29\"\n",
    "]\n",
    "\n",
    "args = parse_args(args)\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52927352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_429209/782281243.py\", line 32, in <module>\n",
      "    logger.info(\"Accelerator state:\", accelerator.state)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1806, in info\n",
      "    self.log(INFO, msg, *args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/accelerate/logging.py\", line 63, in log\n",
      "    self.logger.log(level, msg, *args, **kwargs)\n",
      "Message: 'Accelerator state:'\n",
      "Arguments: (Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      ",)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n",
    "if args.mixed_precision.lower() == \"fp16\":\n",
    "    dtype = torch.float16\n",
    "elif args.mixed_precision.lower() == \"bf16\":\n",
    "    dtype = torch.bfloat16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir,\n",
    "                                                    logging_dir=os.path.join(args.output_dir, \"logs\"))\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)\n",
    "accelerator = Accelerator(mixed_precision=args.mixed_precision,\n",
    "                            project_config=accelerator_project_config,\n",
    "                            kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "# 2.4 Set random seed\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed + accelerator.process_index)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(\"Accelerator state:\", accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addf107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/21/2025 10:37:56 - INFO - __main__ - Number of test examples: 1\n"
     ]
    }
   ],
   "source": [
    "#### Dataset #####\n",
    "# Video data have shape [B, C, F, H, W]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.VideoDataset import *\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "dataset = VideoPathDataset(\n",
    "    source_dir=args.dataset_path,\n",
    ")\n",
    "if args.shuffle:\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=accelerator.num_processes,\n",
    "        rank=accelerator.process_index,\n",
    "        shuffle=True\n",
    "    )\n",
    "else:\n",
    "    sampler = None\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    # sampler=sampler,\n",
    "    collate_fn=lambda x: x,   \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "logger.info(f\"Number of test examples: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec2c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "from model.flameObj import *\n",
    "\n",
    "flamePath = \"/scratch/ondemand28/harryscz/head_audio/head/code/flame/flame2023_no_jaw.npz\"\n",
    "sourcePath = \"/scratch/ondemand28/harryscz/head_audio/head/data/vfhq-fit\"\n",
    "dataPath = [os.path.join(os.path.join(sourcePath, data), \"fit.npz\") for data in os.listdir(sourcePath)]\n",
    "seqPath = \"/scratch/ondemand28/harryscz/head/_-91nXXjrVo_00/fit.npz\"\n",
    "\n",
    "head = Flame(flamePath, device=\"cuda\")\n",
    "\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# for i in tqdm(range(0, len(dataPath), 2)):\n",
    "#     try:\n",
    "#         x = head.batch_uv(dataPath[i:i+2], sample_frames=29)\n",
    "#         assert x.shape[1] == 29\n",
    "#     except:\n",
    "#         print(dataPath[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e24479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video(vae, video):\n",
    "    video = video.to(accelerator.device, dtype=vae.dtype)\n",
    "    video = video.permute(0, 2, 1, 3, 4)  # [B, C, F, H, W]\n",
    "    with torch.no_grad(): latent_dist = vae.encode(video).latent_dist.sample() * vae.config.scaling_factor\n",
    "    return latent_dist.permute(0, 2, 1, 3, 4).to(memory_format=torch.contiguous_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550ca31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b were not used when initializing CAPVideoXTransformer3DModel: \n",
      " ['patch_embed.text_proj.weight, patch_embed.text_proj.bias']\n",
      "Some weights of CAPVideoXTransformer3DModel were not initialized from the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b and are newly initialized: ['patch_embed.ref_temp_proj.bias', 'patch_embed.audio_proj.weight', 'patch_embed.ref_temp_proj.weight', 'patch_embed.cond_proj.bias', 'patch_embed.audio_proj.bias', 'patch_embed.cond_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CAPVideoXTransformer3DModel(\n",
       "  (patch_embed): CAPPatchEmbed(\n",
       "    (proj): Conv2d(16, 1920, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (cond_proj): Conv2d(16, 1920, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (audio_proj): Linear(in_features=3072, out_features=1920, bias=True)\n",
       "    (ref_temp_proj): Linear(in_features=2, out_features=480, bias=True)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=1920, out_features=512, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-29): 30 x CogVideoXBlock(\n",
       "      (norm1): CogVideoXLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=512, out_features=11520, bias=True)\n",
       "        (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attn1): Attention(\n",
       "        (norm_q): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm_k): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_k): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_v): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): CogVideoXLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=512, out_features=11520, bias=True)\n",
       "        (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_final): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_out): AdaLayerNorm(\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=512, out_features=3840, bias=True)\n",
       "    (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1920, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Load Model ####\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXDPMScheduler\n",
    "from model.cap_transformer import CAPVideoXTransformer3DModel\n",
    "\n",
    "transformer = CAPVideoXTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float32,\n",
    "    cond_in_channels=16,  # only one channel (the ref_mask)\n",
    "    sample_width=model_config[\"width\"] // 8,\n",
    "    sample_height=model_config[\"height\"] // 8,\n",
    "    sample_frames=args.sample_frames,\n",
    "    max_text_seq_length=1,\n",
    "    max_n_references=model_config[\"max_n_references\"],\n",
    "    apply_attention_scaling=model_config[\"use_growth_scaling\"],\n",
    "    use_rotary_positional_embeddings=False,\n",
    ")\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "scheduler = CogVideoXDPMScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "vae.eval().to(dtype)\n",
    "transformer.eval().to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04dc676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282910/3094665471.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"/scratch/ondemand28/harryscz/diffusion/modelOut/checkpoint-16000.pt\"\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "transformer.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "vae, transformer, scheduler, data_loader = accelerator.prepare(vae, transformer, scheduler, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371c3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, transformer, scheduler, data_loader = accelerator.prepare(vae, transformer, scheduler, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8adf1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from inference._inference_pipeline import *\n",
    "\n",
    "pipe = CAPVideoPipeline(\n",
    "    vae=vae,\n",
    "    transformer=transformer,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ee30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(data_loader)\n",
    "batch = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b03e807a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/ondemand28/harryscz/head_audio/head/data/vfhq-fit/g1eIAelVFq4_02/fit.npz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c3de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a26064fcf2f402981906d7003b866ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_chunks = []\n",
    "uncond_latent_chunks = []\n",
    "cond_chunks = []\n",
    "uncond_chunks = []\n",
    "cond_vises = []\n",
    "ref_mask_chunks = []\n",
    "\n",
    "uvs = head.batch_uv(batch, resolution=256, rotation=False, sample_frames=args.sample_frames).permute(0,1,4,2,3) # load UVs of shape B, F, C, H, W\n",
    "ref_frame = uvs[:, 0, :, :, :].unsqueeze(1) # B, 1, C, H, W\n",
    "\n",
    "latent_chunk = encode_video(vae, uvs).to(dtype=dtype)\n",
    "_ref_latent = encode_video(vae, ref_frame).to(dtype=dtype)\n",
    "ref_latent = torch.zeros(latent_chunk.shape).to(dtype=dtype, device=accelerator.device)\n",
    "ref_latent[:, 0, ...] = _ref_latent.squeeze(1)\n",
    "ref_mask = torch.zeros(latent_chunk.shape).to(accelerator.device)\n",
    "ref_mask[:, 0, ...] = 1.0\n",
    "\n",
    "# ref_mask_chunks.append(cond_chunk[\"ref_mask\"].permute(0, 1, 4, 2, 3))\n",
    "# cond_vis = conditioning.get_vis(cond_chunk[\"condition\"])\n",
    "# cond_chunk = einops.rearrange(cond_chunk[\"condition\"], 'b f h w c -> b f c h w').to(device=device)\n",
    "# cond_chunk = cond_chunk.to(dtype=dtype)\n",
    "latent_chunks.append(latent_chunk)\n",
    "uncond_latent_chunks.append(latent_chunk * 0.)\n",
    "cond_chunks.append(ref_latent)\n",
    "# cond_vises.append(cond_vis)\n",
    "uncond_chunks.append(ref_latent * 0.)\n",
    "ref_mask_chunks.append(ref_mask)\n",
    "B, F_z, C, H_z, W_z = latent_chunks[0].shape\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     audio_encoding = encode_audio(\n",
    "#         audio_model=audio_model,\n",
    "#         audio=batch[\"audio\"].to(dtype=dtype, device=device),\n",
    "#         sample_positions=batch[\"sample_positions\"].to(dtype=dtype, device=device),\n",
    "#     )\n",
    "# if not batch[\"has_audio\"][0]:  # if there is no audio, set audio encoding to zero\n",
    "#     audio_encoding = audio_encoding * 0.\n",
    "audio_encoding = torch.zeros(\n",
    "    (B, 3, 768), dtype=torch.float32, device=accelerator.device\n",
    ")\n",
    "uncond_audio_encoding = audio_encoding * 0.\n",
    "text_embeds = torch.zeros(1, 1, 1920, device=device, dtype=dtype)\n",
    "\n",
    "sequence_infos = []\n",
    "for chunk_id, latent in enumerate(latent_chunks):\n",
    "    sequence_infos.append((False, torch.arange(0, latent.shape[1], device=accelerator.device)))\n",
    "\n",
    "out = pipe(\n",
    "    height=256,\n",
    "    width=256,\n",
    "    num_frames=29,\n",
    "    num_inference_steps=50,\n",
    "    conditioning=cond_chunks,\n",
    "    uncond_conditioning=uncond_chunks,\n",
    "    latents=latent_chunks,\n",
    "    uncond_latents=uncond_latent_chunks,\n",
    "    ref_mask_chunks=ref_mask_chunks,\n",
    "    audio_embeds=audio_encoding,\n",
    "    uncond_audio_embeds=uncond_audio_encoding,\n",
    "    text_embeds=text_embeds,\n",
    "    uncond_text_embeds=text_embeds,\n",
    "    sequence_infos=sequence_infos, \n",
    "    output_type=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "795a2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfb30e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2236,  0.0980, -2.5376],\n",
       "         ...,\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684]],\n",
       "\n",
       "        [[ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2236,  0.0980, -2.5376],\n",
       "         ...,\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684]],\n",
       "\n",
       "        [[ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2242,  0.0974, -2.5384],\n",
       "         [ 0.2236,  0.0980, -2.5376],\n",
       "         ...,\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684],\n",
       "         [ 0.1003,  0.1259, -2.4684]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2284,  0.0986, -2.5359],\n",
       "         [ 0.2284,  0.0986, -2.5359],\n",
       "         [ 0.2276,  0.0989, -2.5356],\n",
       "         ...,\n",
       "         [ 0.1043,  0.1250, -2.4653],\n",
       "         [ 0.1043,  0.1250, -2.4653],\n",
       "         [ 0.1043,  0.1250, -2.4653]],\n",
       "\n",
       "        [[ 0.2293,  0.0983, -2.5362],\n",
       "         [ 0.2293,  0.0983, -2.5362],\n",
       "         [ 0.2282,  0.0992, -2.5362],\n",
       "         ...,\n",
       "         [ 0.1042,  0.1248, -2.4653],\n",
       "         [ 0.1042,  0.1248, -2.4653],\n",
       "         [ 0.1042,  0.1248, -2.4653]],\n",
       "\n",
       "        [[ 0.2296,  0.0986, -2.5362],\n",
       "         [ 0.2296,  0.0986, -2.5362],\n",
       "         [ 0.2285,  0.0992, -2.5360],\n",
       "         ...,\n",
       "         [ 0.1039,  0.1246, -2.4655],\n",
       "         [ 0.1039,  0.1246, -2.4655],\n",
       "         [ 0.1039,  0.1246, -2.4655]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "uvs = head.batch_uv(dataPath[:1], resolution=256, rotation=False, sample_frames=args.sample_frames).permute(0,1,4,2,3) # load UVs of shape B, F, C, H, W\n",
    "orig = uvs[i].permute(0,2,3,1)\n",
    "\n",
    "recon = out[0][0][0].permute(0,2,3,1)\n",
    "\n",
    "oUV = head.sampleFromUV(orig, savePath=f\"diffOut/{i}_uv.mp4\") # input has shape F, H, W, C\n",
    "head.sampleTo3D(oUV, savePath=f\"diffOut/{i}_3d.mp4\", resolution=512, dist=1.2, azim=20, elev=10)\n",
    "\n",
    "diffUV = head.sampleFromUV(recon, savePath=f\"diffOut/{i}_diff_uv.mp4\") # input has shape F, H, W, C\n",
    "head.sampleTo3D(diffUV, savePath=f\"diffOut/{i}_diff_3d.mp4\", resolution=512, dist=1.2, azim=20, elev=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e76f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0893, -0.2974, -3.0125],\n",
       "         ...,\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698]],\n",
       "\n",
       "        [[-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0893, -0.2974, -3.0125],\n",
       "         ...,\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698]],\n",
       "\n",
       "        [[-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0878, -0.2992, -3.0141],\n",
       "         [-0.0893, -0.2974, -3.0125],\n",
       "         ...,\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698],\n",
       "         [-0.3341, -0.2423, -2.8698]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0845, -0.2977, -3.0104],\n",
       "         [-0.0845, -0.2977, -3.0104],\n",
       "         [-0.0854, -0.2955, -3.0094],\n",
       "         ...,\n",
       "         [-0.3335, -0.2420, -2.8668],\n",
       "         [-0.3335, -0.2420, -2.8668],\n",
       "         [-0.3335, -0.2420, -2.8668]],\n",
       "\n",
       "        [[-0.0839, -0.2978, -3.0094],\n",
       "         [-0.0839, -0.2978, -3.0094],\n",
       "         [-0.0850, -0.2954, -3.0095],\n",
       "         ...,\n",
       "         [-0.3331, -0.2414, -2.8646],\n",
       "         [-0.3331, -0.2414, -2.8646],\n",
       "         [-0.3331, -0.2414, -2.8646]],\n",
       "\n",
       "        [[-0.0839, -0.2980, -3.0093],\n",
       "         [-0.0839, -0.2980, -3.0093],\n",
       "         [-0.0851, -0.2958, -3.0093],\n",
       "         ...,\n",
       "         [-0.3335, -0.2415, -2.8646],\n",
       "         [-0.3335, -0.2415, -2.8646],\n",
       "         [-0.3335, -0.2415, -2.8646]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_latents(vae, latents: torch.Tensor) -> torch.Tensor:\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)  # [batch_size, num_channels, num_frames, height, width]\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "\n",
    "    with torch.no_grad(): frames = vae.decode(latents).sample\n",
    "    return frames\n",
    "i = 0\n",
    "uvs = head.batch_uv(dataPath[:1], resolution=256, rotation=False, sample_frames=args.sample_frames).permute(0,1,4,2,3) # load UVs of shape B, F, C, H, W\n",
    "z = encode_video(vae, uvs)\n",
    "y = decode_latents(vae,z)[0].permute(1,2,3,0)\n",
    "\n",
    "diffUV = head.sampleFromUV(y, savePath=f\"diffOut/{i}_vae_uv.mp4\") # input has shape F, H, W, C\n",
    "head.sampleTo3D(diffUV, savePath=f\"diffOut/{i}_vae_3d.mp4\", resolution=512, dist=1.2, azim=20, elev=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96915c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 256, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d03eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0865, -0.2964, -3.0184],\n",
       "         [-0.0858, -0.2966, -3.0159],\n",
       "         [-0.0858, -0.2954, -3.0154],\n",
       "         ...,\n",
       "         [-0.3338, -0.2333, -2.8628],\n",
       "         [-0.3301, -0.2330, -2.8595],\n",
       "         [-0.3301, -0.2330, -2.8595]],\n",
       "\n",
       "        [[-0.0866, -0.2964, -3.0185],\n",
       "         [-0.0860, -0.2966, -3.0160],\n",
       "         [-0.0860, -0.2954, -3.0154],\n",
       "         ...,\n",
       "         [-0.3339, -0.2332, -2.8628],\n",
       "         [-0.3302, -0.2329, -2.8594],\n",
       "         [-0.3302, -0.2329, -2.8594]],\n",
       "\n",
       "        [[-0.0867, -0.2963, -3.0185],\n",
       "         [-0.0861, -0.2966, -3.0160],\n",
       "         [-0.0861, -0.2954, -3.0155],\n",
       "         ...,\n",
       "         [-0.3342, -0.2333, -2.8629],\n",
       "         [-0.3305, -0.2331, -2.8596],\n",
       "         [-0.3305, -0.2331, -2.8596]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0866, -0.2960, -3.0177],\n",
       "         [-0.0859, -0.2962, -3.0152],\n",
       "         [-0.0859, -0.2950, -3.0146],\n",
       "         ...,\n",
       "         [-0.3328, -0.2328, -2.8617],\n",
       "         [-0.3291, -0.2325, -2.8584],\n",
       "         [-0.3291, -0.2325, -2.8584]],\n",
       "\n",
       "        [[-0.0865, -0.2960, -3.0177],\n",
       "         [-0.0858, -0.2963, -3.0152],\n",
       "         [-0.0858, -0.2951, -3.0146],\n",
       "         ...,\n",
       "         [-0.3328, -0.2327, -2.8617],\n",
       "         [-0.3291, -0.2325, -2.8584],\n",
       "         [-0.3291, -0.2325, -2.8584]],\n",
       "\n",
       "        [[-0.0866, -0.2960, -3.0177],\n",
       "         [-0.0859, -0.2963, -3.0152],\n",
       "         [-0.0859, -0.2951, -3.0147],\n",
       "         ...,\n",
       "         [-0.3328, -0.2329, -2.8615],\n",
       "         [-0.3291, -0.2327, -2.8582],\n",
       "         [-0.3291, -0.2327, -2.8582]]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "head.loadSequence(dataPath[i])\n",
    "origMesh = head.LSB(rotation=False)\n",
    "# head.renderVertsAnimation(savePath=f\"flameOut/{i}orig.mp4\", resolution=512)\n",
    "uvMesh = head.convertUV(rotation=False)\n",
    "uvs = head.get_uv_animation(uvMesh, savePath=f\"flameOut/{i}uv.mp4\", resolution=512)\n",
    "sampledUV = head.sampleFromUV(uvs, savePath=f\"flameOut/{i}sample.mp4\", resolution=512)\n",
    "head.sampleTo3D(sampledUV, savePath=f\"flameOut/{i}recon.mp4\", resolution=512, dist=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee41b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 512, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0][0].permute(1,0,2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76e5e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video_tensor, path):\n",
    "    video_tensor = video_tensor.squeeze(0)  # [3, 49, 256, 256]\n",
    "    video_tensor = video_tensor.permute(1, 2, 3, 0)  # [49, 256, 256, 3]\n",
    "\n",
    "    # Normalize to 0–255 and convert to uint8 if needed\n",
    "    video_np = (video_tensor * 255).clamp(0, 255).byte().cpu().numpy()\n",
    "\n",
    "    # Save as .mp4\n",
    "    imageio.mimsave(path, video_np, fps=16)\n",
    "\n",
    "save_video(out[0][0][0].permute(1,0,2,3) , \"/scratch/ondemand28/harryscz/diffusion/videoOut/ref1kUV3.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc61374",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m head\u001b[38;5;241m.\u001b[39mloadSequence(dataPath[\u001b[38;5;241m1\u001b[39m])                                       \u001b[38;5;66;03m# PASS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m perFrameVerts \u001b[38;5;241m=\u001b[39m head\u001b[38;5;241m.\u001b[39mLSB()                                           \u001b[38;5;66;03m# PASS\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderVertsAnimation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache/original3d.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomVerts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperFrameVerts\u001b[49m\u001b[43m)\u001b[49m                         \u001b[38;5;66;03m# PASS\u001b[39;00m\n\u001b[1;32m      4\u001b[0m uvMesh \u001b[38;5;241m=\u001b[39m head\u001b[38;5;241m.\u001b[39mconvertUV()                                            \u001b[38;5;66;03m# CHECKED\u001b[39;00m\n\u001b[1;32m      5\u001b[0m perFrameUV \u001b[38;5;241m=\u001b[39m head\u001b[38;5;241m.\u001b[39mget_uv_animation(uvMesh, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache/originalUV.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)           \u001b[38;5;66;03m# ___\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/diffusion/model/flameObj.py:331\u001b[0m, in \u001b[0;36mFlame.renderVertsAnimation\u001b[0;34m(self, savePath, vertsTexture, customVerts, resolution, dist)\u001b[0m\n\u001b[1;32m    324\u001b[0m lights \u001b[38;5;241m=\u001b[39m PointLights(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, location\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;241m10.0\u001b[39m]])\n\u001b[1;32m    326\u001b[0m renderer \u001b[38;5;241m=\u001b[39m MeshRenderer(\n\u001b[1;32m    327\u001b[0m     rasterizer\u001b[38;5;241m=\u001b[39mMeshRasterizer(cameras\u001b[38;5;241m=\u001b[39mcameras, raster_settings\u001b[38;5;241m=\u001b[39mraster_settings),\n\u001b[1;32m    328\u001b[0m     shader\u001b[38;5;241m=\u001b[39mSoftPhongShader(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, cameras\u001b[38;5;241m=\u001b[39mcameras, lights\u001b[38;5;241m=\u001b[39mlights)\n\u001b[1;32m    329\u001b[0m )\n\u001b[0;32m--> 331\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadMesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m frames \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# shape: (B, H, W, 3)\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Convert from [0, 1] floats to [0, 255] uint8 if needed:\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/renderer/mesh/renderer.py:64\u001b[0m, in \u001b[0;36mMeshRenderer.forward\u001b[0;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mRender a batch of images from a batch of meshes by rasterizing and then\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mshading.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mFor this set rasterizer.raster_settings.clip_barycentric_coords=True\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m fragments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrasterizer(meshes_world, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeshes_world\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/renderer/mesh/shader.py:128\u001b[0m, in \u001b[0;36mSoftPhongShader.forward\u001b[0;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, fragments: Fragments, meshes: Meshes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    127\u001b[0m     cameras \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_get_cameras(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 128\u001b[0m     texels \u001b[38;5;241m=\u001b[39m \u001b[43mmeshes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_textures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     lights \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlights)\n\u001b[1;32m    130\u001b[0m     materials \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaterials\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaterials)\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/structures/meshes.py:1544\u001b[0m, in \u001b[0;36mMeshes.sample_textures\u001b[0;34m(self, fragments)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# Pass in faces packed. If the textures are defined per\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;66;03m# vertex, the face indices are needed in order to interpolate\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;66;03m# the vertex attributes across the face.\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtextures\u001b[38;5;241m.\u001b[39msample_textures(\n\u001b[0;32m-> 1544\u001b[0m         fragments, faces_packed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfaces_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m     )\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeshes does not have textures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/structures/meshes.py:609\u001b[0m, in \u001b[0;36mMeshes.faces_packed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfaces_packed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    602\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m    Get the packed representation of the faces.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m    Faces are given by the indices of the three vertices in verts_packed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m        tensor of faces of shape (sum(F_n), 3).\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_faces_packed\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/structures/meshes.py:976\u001b[0m, in \u001b[0;36mMeshes._compute_packed\u001b[0;34m(self, refresh)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# Packed can be calculated from padded or list, so can call the\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# accessor function for verts_list and faces_list.\u001b[39;00m\n\u001b[0;32m--> 976\u001b[0m verts_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverts_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m faces_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaces_list()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misempty():\n",
      "File \u001b[0;32m/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/structures/meshes.py:538\u001b[0m, in \u001b[0;36mMeshes.verts_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_padded \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverts_padded is required to compute verts_list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_list \u001b[38;5;241m=\u001b[39m struct_utils\u001b[38;5;241m.\u001b[39mpadded_to_list(\n\u001b[0;32m--> 538\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_padded, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_verts_per_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_list\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "head.loadSequence(dataPath[1])                                       # PASS\n",
    "perFrameVerts = head.LSB()                                           # PASS\n",
    "# head.renderVertsAnimation(\"cache/original3d.mp4\", customVerts=perFrameVerts)                         # PASS\n",
    "uvMesh = head.convertUV(rotation=False)                                            # CHECKED\n",
    "perFrameUV = head.get_uv_animation(uvMesh, \"cache/originalUV.mp4\", fill=True, resolution=512)           # ___\n",
    "sampledUV = head.sampleFromUV(perFrameUV, \"cache/sampledUV.mp4\", resolution=512)     # ___\n",
    "sampled3dSeq = head.sampleTo3D(sampledUV, \"cache/sampled3d.mp4\", dist=1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e7554",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc989756",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv = out[0][0][0].permute(0,2,3,1)\n",
    "sampledUV = head.sampleFromUV(uv, \"videoOut/ref1kUV3.mp4\", resolution=256) # Input need to be N, W, H, C\n",
    "sampled3dSeq = head.sampleTo3D(sampledUV, \"videoOut/ref1kUV33d.mp4\", dist=1, scale=1.6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c41a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "vid_dir = os.listdir(\"/scratch/ondemand28/harryscz/head_audio/data/data256/uv/\")\n",
    "vid_path = os.path.join(\"/scratch/ondemand28/harryscz/head_audio/data/data256/uv/\", vid_dir[i])\n",
    "\n",
    "video, audio, info = torchvision.io.read_video(vid_path, pts_unit='sec')\n",
    "video = (video.float() / 255).to(\"cuda\")[:29 , ...]\n",
    "\n",
    "frames_rgb = (video * 255).byte().cpu().numpy()  \n",
    "imageio.mimsave(f\"videoOut/{i}_OrigUV.mp4\", frames_rgb, fps=16, quality=10) \n",
    "\n",
    "sampledUV = head.sampleFromUV(video, f\"videoOut/{i}_SampledUV.mp4\", resolution=256) # Input need to be N, W, H, C\n",
    "sampled3dSeq = head.sampleTo3D(sampledUV, f\"videoOut/{i}_recon3d.mp4\", dist=1, scale=1.6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04d3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "head.loadSequence(dataPath[20])\n",
    "head.LSB()\n",
    "uvMesh = head.convertUV()\n",
    "uv = head.get_uv_animation(uvMesh, savePath=f\"videoOut/20_uv.mp4\", resolution=256)\n",
    "torch.save(uv, \"videoOut/20.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe46a6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/nfs/horai.dgpsrv/ondemand28/harryscz/diffusion/data/VideoDataset.py\", line 55, in __getitem__\n    head.loadSequence(path)\nNameError: name 'head' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(data_loader)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mNameError\u001b[0m: Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/nfs/horai.dgpsrv/ondemand28/harryscz/diffusion/data/VideoDataset.py\", line 55, in __getitem__\n    head.loadSequence(path)\nNameError: name 'head' is not defined\n"
     ]
    }
   ],
   "source": [
    "data = iter(data_loader)\n",
    "next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ffdf7422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2226707/2551400825.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  uv = torch.load(\"videoOut/20.pt\")\n"
     ]
    }
   ],
   "source": [
    "uv = torch.load(\"videoOut/20.pt\")\n",
    "sampledUV = head.sampleFromUV(uv, \"videoOut/20uv.mp4\", resolution=256) # Input need to be N, W, H, C\n",
    "sampled3dSeq = head.sampleTo3D(sampledUV, \"videoOut/203d.mp4\", dist=1, scale=1.6)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505e8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
