{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03351ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random     \n",
    "import inspect\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from diffusers import DiffusionPipeline, CogVideoXDDIMScheduler\n",
    "from diffusers.models import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63813d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/horai.dgpsrv/ondemand28/harryscz/pytorch3d/pytorch3d/io/obj_io.py:551: UserWarning: Mtl file does not exist: /scratch/ondemand28/harryscz/head_audio/head/template.mtl\n",
      "  warnings.warn(f\"Mtl file does not exist: {f}\")\n"
     ]
    }
   ],
   "source": [
    "if 'flameObj' in sys.modules:\n",
    "    del sys.modules['flameObj']\n",
    "\n",
    "from flameObj import *\n",
    "\n",
    "flamePath = flamePath = \"/scratch/ondemand28/harryscz/head_audio/head/code/flame/flame2023_no_jaw.npz\"\n",
    "sourcePath = \"/scratch/ondemand28/harryscz/head_audio/head/data/vfhq-fit\"\n",
    "dataPath = [os.path.join(os.path.join(sourcePath, data), \"fit.npz\") for data in os.listdir(sourcePath)]\n",
    "seqPath = \"/scratch/ondemand28/harryscz/head_aduiohead/_-91nXXjrVo_00/fit.npz\"\n",
    "\n",
    "head = Flame(flamePath, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb0ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXDPMScheduler\n",
    "from cap_transformer import CAPVideoXTransformer3DModel\n",
    "import yaml\n",
    "\n",
    "weight_dtype=torch.float32\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fabb4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b were not used when initializing CAPVideoXTransformer3DModel: \n",
      " ['patch_embed.text_proj.bias, patch_embed.text_proj.weight']\n",
      "Some weights of CAPVideoXTransformer3DModel were not initialized from the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b and are newly initialized: ['patch_embed.ref_temp_proj.bias', 'patch_embed.cond_proj.bias', 'patch_embed.audio_proj.weight', 'patch_embed.cond_proj.weight', 'patch_embed.audio_proj.bias', 'patch_embed.ref_temp_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1866141/425443757.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Missing keys (these will be randomly initialized because they weren't in the checkpoint):\n",
      "==> Unexpected keys (these were in the checkpoint but didn't match any parameter in your model):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CAPVideoXTransformer3DModel(\n",
       "  (patch_embed): CAPPatchEmbed(\n",
       "    (proj): Conv2d(16, 1920, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (cond_proj): Conv2d(1, 1920, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (audio_proj): Linear(in_features=3072, out_features=1920, bias=True)\n",
       "    (ref_temp_proj): Linear(in_features=2, out_features=480, bias=True)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=1920, out_features=512, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-29): 30 x CogVideoXBlock(\n",
       "      (norm1): CogVideoXLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=512, out_features=11520, bias=True)\n",
       "        (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attn1): Attention(\n",
       "        (norm_q): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm_k): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_k): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_v): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1920, out_features=1920, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): CogVideoXLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=512, out_features=11520, bias=True)\n",
       "        (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1920, out_features=7680, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=7680, out_features=1920, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_final): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_out): AdaLayerNorm(\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=512, out_features=3840, bias=True)\n",
       "    (norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1920, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config=\"/scratch/ondemand28/harryscz/diffusion/model_config.yaml\"\n",
    "pretrained_model_name_or_path = \"/scratch/ondemand28/harryscz/model/CogVideoX-2b\"\n",
    "ckpt_path = \"/scratch/ondemand28/harryscz/head_audio/trainOutput/checkpoint-1000.pt\"\n",
    "\n",
    "with open(model_config) as f: model_config_yaml = yaml.safe_load(f)\n",
    "\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "scheduler = CogVideoXDDIMScheduler.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "transformer = CAPVideoXTransformer3DModel.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float32,\n",
    "    cond_in_channels=1,  # only one channel (the ref_mask)\n",
    "    sample_width=model_config_yaml[\"width\"] // 8,\n",
    "    sample_height=model_config_yaml[\"height\"] // 8,\n",
    "    max_text_seq_length=1,\n",
    "    max_n_references=model_config_yaml[\"max_n_references\"],\n",
    "    apply_attention_scaling=model_config_yaml[\"use_growth_scaling\"],\n",
    "    use_rotary_positional_embeddings=False,\n",
    ")\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "if \"state_dict\" in ckpt:\n",
    "    raw_state_dict = ckpt[\"state_dict\"]\n",
    "elif \"model_state_dict\" in ckpt:\n",
    "    raw_state_dict = ckpt[\"model_state_dict\"]\n",
    "else:\n",
    "    # If the .pt is literally just a pure state_dict, do this:\n",
    "    raw_state_dict = ckpt\n",
    "\n",
    "clean_state_dict = {}\n",
    "for key, val in raw_state_dict.items():\n",
    "    new_key = key\n",
    "    # e.g. if your keys start with \"module.\", remove it:\n",
    "    if key.startswith(\"module.\"):\n",
    "        new_key = key[len(\"module.\"):]\n",
    "    # or if saved under \"model.\", do:\n",
    "    # if key.startswith(\"model.\"):\n",
    "    #     new_key = key[len(\"model.\"):]\n",
    "    clean_state_dict[new_key] = val\n",
    "\n",
    "missing, unexpected = transformer.load_state_dict(clean_state_dict, strict=False)\n",
    "\n",
    "print(\"==> Missing keys (these will be randomly initialized because they weren't in the checkpoint):\")\n",
    "for k in missing:\n",
    "    print(\"   \", k)\n",
    "print(\"==> Unexpected keys (these were in the checkpoint but didn't match any parameter in your model):\")\n",
    "for k in unexpected:\n",
    "    print(\"   \", k)\n",
    "\n",
    "vae.eval().to(device)\n",
    "transformer.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245cef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_frame(pil_frame, target_resolution=(model_config_yaml[\"height\"], model_config_yaml[\"width\"])):\n",
    "    \"\"\"\n",
    "    1) Resize the input PIL frame\n",
    "    2) Convert to floatTensor in [-1, +1]\n",
    "    3) Return a (1, 3, H, W) tensor on CPU\n",
    "    \"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(target_resolution, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),            # → [0,1]\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),  # → [-1, +1]\n",
    "    ])\n",
    "    x = preprocess(pil_frame)  # shape: (3, H, W), CPU float32\n",
    "    return x.unsqueeze(0)      # shape: (1, 3, H, W)\n",
    "\n",
    "def encode_single_frame(vae, frame_tensor):\n",
    "    \"\"\"\n",
    "    frame_tensor: (1, 3, H, W), in [-1, +1], CPU or device\n",
    "    returns: latent of shape (1, C_z, F_lat=1, h_lat, w_lat)\n",
    "    \"\"\"\n",
    "    frame_tensor = frame_tensor.to(device=device, dtype=weight_dtype)\n",
    "    with torch.no_grad():\n",
    "        # Note: CogVideoX’s VAE expects a 5D “video” tensor: (B, 3, F, H, W).\n",
    "        # For a single frame, F=1.\n",
    "        video_input = frame_tensor.unsqueeze(2)  # → (1, 3, 1, H, W)\n",
    "        z = vae.encode(video_input).latent_dist.sample()  # (1, C_z, 1, h_lat, w_lat), float32\n",
    "        z = z * vae.config.scaling_factor\n",
    "    return z.contiguous()  \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((model_config_yaml[\"height\"], model_config_yaml[\"width\"]),\n",
    "                      interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),  # → [-1,+1]\n",
    "])\n",
    "\n",
    "def preprocess_frame_to_tensor(pil_img):\n",
    "    x = preprocess(pil_img)         # (3, H, W) in [-1,+1]\n",
    "    return x.unsqueeze(0)           # (1, 3, H, W)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_single_frame(vae, frame_tensor):\n",
    "    # frame_tensor: (1,3,H,W) in [-1,+1], CPU or already on device\n",
    "    f = frame_tensor.to(device, dtype=weight_dtype)\n",
    "    video_in = f.unsqueeze(2)       # (1,3,1,H,W)\n",
    "    with torch.no_grad():\n",
    "        z = vae.encode(video_in).latent_dist.sample()  # (1, C_z, 1, h_lat, w_lat)\n",
    "        z = z * vae.config.scaling_factor\n",
    "    return z.contiguous()           # (1, C_z, 1, h_lat, w_lat)\n",
    "\n",
    "# (C) Build the masks, embeddings & sequence_infos for inference\n",
    "\n",
    "def build_masks_and_embeddings(latent_ref):\n",
    "    \"\"\"\n",
    "    latent_ref: (1, C_z, 1, h_lat, w_lat)\n",
    "    returns:\n",
    "      cond_chunks:      [ (1, 1, 1, h_lat, w_lat) ]\n",
    "      sequence_infos:   [ (False, tensor([0], device=device)) ]\n",
    "      fake_text_embeds: (1, 1, inner_dim)\n",
    "      fake_audio_embeds: (1, 1, 768)\n",
    "    \"\"\"\n",
    "    B, C_z, F_lat, h_lat, w_lat = latent_ref.shape\n",
    "    assert B == 1 and F_lat == 1\n",
    "\n",
    "    # C)1) Reference mask = all‐ones\n",
    "    ref_mask_latent = torch.ones((B, 1, F_lat, h_lat, w_lat), device=device, dtype=weight_dtype)\n",
    "    cond_mask_chunk = ref_mask_latent.permute(0, 2, 1, 3, 4)  # (1,1,1,h_lat,w_lat)\n",
    "    cond_chunks = [cond_mask_chunk]\n",
    "\n",
    "    # C)2) sequence_infos\n",
    "    seq_idx = torch.arange(0, F_lat, device=device)  # → tensor([0])\n",
    "    sequence_infos = [(False, seq_idx)]\n",
    "\n",
    "    # C)3) fake_text_embeds\n",
    "    inner_dim = transformer.config.num_attention_heads * transformer.config.attention_head_dim\n",
    "    fake_text_embeds = torch.zeros((B, 1, inner_dim), device=device, dtype=weight_dtype)\n",
    "\n",
    "    # C)4) fake_audio_embeds\n",
    "    audio_feature_dim = 768\n",
    "    fake_audio_embeds = torch.zeros((B, F_lat, audio_feature_dim), device=device, dtype=weight_dtype)\n",
    "\n",
    "    return cond_chunks, sequence_infos, fake_text_embeds, fake_audio_embeds\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoise_from_single_frame(\n",
    "    vae,\n",
    "    transformer,\n",
    "    scheduler,\n",
    "    latent_ref,            # (1, C_z, 1, h_lat, w_lat)\n",
    "    cond_chunks,           # [ (1, 1, 1, h_lat, w_lat) ]\n",
    "    sequence_infos,        # [ (False, tensor([0])) ]\n",
    "    fake_text_embeds,      # (1, 1, inner_dim)\n",
    "    fake_audio_embeds,     # (1, 1, 768)\n",
    "    num_inference_steps=50,\n",
    "):\n",
    "    B, C_z, F_lat, h_lat, w_lat = latent_ref.shape\n",
    "    assert B == 1 and F_lat == 1\n",
    "\n",
    "    # D.1) Start from pure noise in latent space\n",
    "    current_latent = torch.randn_like(latent_ref, device=device, dtype=weight_dtype)\n",
    "\n",
    "    # D.2) Prepare timesteps (linear spacing from T-1 down to 0)\n",
    "    scheduler.set_format(\"pt\")\n",
    "    T = scheduler.config.num_train_timesteps\n",
    "    timesteps = torch.linspace(T - 1, 0, num_inference_steps, dtype=torch.long, device=device)\n",
    "\n",
    "    # D.3) Reverse‐diffusion loop\n",
    "    for t in timesteps:\n",
    "        # (1) Permute current_latent into the shape (B, F_lat, C_z, h_lat, w_lat)\n",
    "        noised_chunk = current_latent.permute(0, 2, 1, 3, 4)  # → (1,1,C_z,h_lat,w_lat)\n",
    "\n",
    "        # (2) Infer velocity with the transformer\n",
    "        out_list = transformer(\n",
    "            hidden_states=[noised_chunk],\n",
    "            condition=cond_chunks,\n",
    "            sequence_infos=sequence_infos,\n",
    "            timestep=torch.tensor([t], device=device),\n",
    "            audio_embeds=fake_audio_embeds,\n",
    "            encoder_hidden_states=fake_text_embeds,\n",
    "            image_rotary_emb=None,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        # out_list is a list of length 1; element[0].shape = (1,1,C_z,h_lat,w_lat)\n",
    "\n",
    "        pred_velocity = torch.cat(out_list, dim=1)  # → (1,1,C_z,h_lat,w_lat)\n",
    "\n",
    "        # (3) One scheduler.step call → produce the “previous sample” at t-1\n",
    "        step_output = scheduler.step(pred_velocity, t, current_latent)\n",
    "        current_latent = step_output.prev_sample\n",
    "\n",
    "    # D.4) Invert the VAE scaling to get the final “clean” latent\n",
    "    final_latent = current_latent / vae.config.scaling_factor\n",
    "\n",
    "    # D.5) Decode with VAE\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(final_latent).sample  # (1,3,1,H,W)\n",
    "        decoded_image = decoded.squeeze(2)          # (1,3,H,W)\n",
    "\n",
    "    return decoded_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca78dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
