{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b0d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from CustomDataset import VideoDataset\n",
    "from cap_transformer import CAPVideoXTransformer3DModel\n",
    "from cap_transformer import CAPVideoXTransformer3DModel\n",
    "from trainUtils import *\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXDPMScheduler\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1127264",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/scratch/ondemand28/harryscz/head_audio/data/data256/uv\"\n",
    "pretrained_model_name_or_path = \"/scratch/ondemand28/harryscz/model/CogVideoX-2b\"\n",
    "gradient_accumulation_steps =  2\n",
    "mixed_precision = 'bf16'\n",
    "report_to = \"wandb\"\n",
    "logging_dir = \"/scratch/ondemand28/harryscz/head_audio/trainlog\"\n",
    "output_dir = \"/scratch/ondemand28/harryscz/head_audio/trainOutput\"\n",
    "report_to = \"wandb\"\n",
    "seed = 42\n",
    "batch_size = 1\n",
    "use_text = False\n",
    "model_config = \"/scratch/ondemand28/harryscz/diffusion/model_config.yaml\"\n",
    "revision = None\n",
    "variant = None\n",
    "enable_slicing = True\n",
    "enable_tiling = True\n",
    "gradient_accumulation_steps = 2\n",
    "max_train_steps = None\n",
    "num_train_epochs = 1\n",
    "lr_scheduler = \"cosine\"\n",
    "lr_warmup_steps = 500\n",
    "lr_num_cycles = 1\n",
    "lr_power=1.0\n",
    "learning_rate=1e-4\n",
    "tracker_name = \"cogvideox\"\n",
    "is_uncond = False\n",
    "max_grad_norm = 1\n",
    "checkpointing_steps = 10\n",
    "\n",
    "with open(model_config) as f: model_config_yaml = yaml.safe_load(f)\n",
    "\n",
    "weight_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3139df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(project_dir=output_dir, logging_dir=logging_dir)\n",
    "kwargs = DistributedDataParallelKwargs()\n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "        log_with=report_to,\n",
    "        project_config=accelerator_project_config,\n",
    "        kwargs_handlers=[kwargs],\n",
    "    )\n",
    "\n",
    "if seed is not None:\n",
    "    set_seed(seed + accelerator.process_index)\n",
    "    print(\"Seed:\", seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b04d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/01/2025 16:37:30 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(__name__)\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    from transformers import logging as hf_logging\n",
    "    hf_logging.set_verbosity_warning()\n",
    "    import diffusers\n",
    "    hf_logging.set_verbosity_info()\n",
    "else:\n",
    "    from transformers import logging as hf_logging\n",
    "    hf_logging.set_verbosity_error()\n",
    "    import diffusers\n",
    "    hf_logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7aeb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    seed = torch.initial_seed() % (2**32)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "train_dataset = VideoDataset(\n",
    "    videos_dir=dataset_path,\n",
    "    num_ref_frames=1,\n",
    "    num_target_frames=49\n",
    ")\n",
    "sampler = DistributedSampler(\n",
    "    train_dataset,\n",
    "    num_replicas=accelerator.num_processes,\n",
    "    rank=accelerator.process_index,\n",
    "    shuffle=True\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    collate_fn=lambda x: x[0],\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    generator=g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3ba49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b were not used when initializing CAPVideoXTransformer3DModel: \n",
      " ['patch_embed.text_proj.bias, patch_embed.text_proj.weight']\n",
      "Some weights of CAPVideoXTransformer3DModel were not initialized from the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b and are newly initialized: ['patch_embed.ref_temp_proj.bias', 'patch_embed.ref_temp_proj.weight', 'patch_embed.cond_proj.bias', 'patch_embed.audio_proj.weight', 'patch_embed.cond_proj.weight', 'patch_embed.audio_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if use_text:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n",
    "    )\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
    "    )\n",
    "    text_encoder.requires_grad_(False)\n",
    "else:\n",
    "    tokenizer = None\n",
    "    text_encoder = None\n",
    "\n",
    "if model_config_yaml[\"use_audio\"]:\n",
    "    audio_model = Wav2Vec2Model.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    audio_model.freeze_feature_encoder()\n",
    "    audio_model.encoder.config.layerdrop = 0.\n",
    "    audio_model.requires_grad_(True)\n",
    "else:\n",
    "    audio_model = None\n",
    "\n",
    "load_dtype = torch.bfloat16 if \"5b\" in pretrained_model_name_or_path.lower() else torch.float16\n",
    "\n",
    "transformer = CAPVideoXTransformer3DModel.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float32,\n",
    "    revision=revision,\n",
    "    variant=variant,\n",
    "    cond_in_channels=1,  # only one channel (the ref_mask)\n",
    "    sample_width=model_config_yaml[\"width\"] // 8,\n",
    "    sample_height=model_config_yaml[\"height\"] // 8,\n",
    "    max_text_seq_length=1,\n",
    "    max_n_references=model_config_yaml[\"max_n_references\"],\n",
    "    apply_attention_scaling=model_config_yaml[\"use_growth_scaling\"],\n",
    "    use_rotary_positional_embeddings=False,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"vae\",\n",
    "    revision=revision,\n",
    "    variant=variant,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "scheduler = CogVideoXDPMScheduler.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"scheduler\",\n",
    ")\n",
    "\n",
    "if enable_slicing:\n",
    "    vae.enable_slicing()\n",
    "if enable_tiling:\n",
    "    vae.enable_tiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c07f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a9a8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKLCogVideoX(\n",
       "  (encoder): CogVideoXEncoder3D(\n",
       "    (conv_in): CogVideoXCausalConv3d(\n",
       "      (conv): CogVideoXSafeConv3d(3, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    )\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): CogVideoXDownBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): CogVideoXDownsample3D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CogVideoXDownBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (conv_shortcut): CogVideoXSafeConv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (1-2): 2 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): CogVideoXDownsample3D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CogVideoXDownBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): CogVideoXDownsample3D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CogVideoXDownBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (conv_shortcut): CogVideoXSafeConv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (1-2): 2 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): CogVideoXMidBlock3D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x CogVideoXResnetBlock3D(\n",
       "          (nonlinearity): SiLU()\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): CogVideoXCausalConv3d(\n",
       "            (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): CogVideoXCausalConv3d(\n",
       "            (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): CogVideoXCausalConv3d(\n",
       "      (conv): CogVideoXSafeConv3d(512, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (decoder): CogVideoXDecoder3D(\n",
       "    (conv_in): CogVideoXCausalConv3d(\n",
       "      (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    )\n",
       "    (mid_block): CogVideoXMidBlock3D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x CogVideoXResnetBlock3D(\n",
       "          (nonlinearity): SiLU()\n",
       "          (norm1): CogVideoXSpatialNorm3D(\n",
       "            (norm_layer): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv_y): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "            )\n",
       "            (conv_b): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "            )\n",
       "          )\n",
       "          (norm2): CogVideoXSpatialNorm3D(\n",
       "            (norm_layer): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv_y): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "            )\n",
       "            (conv_b): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "            )\n",
       "          )\n",
       "          (conv1): CogVideoXCausalConv3d(\n",
       "            (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): CogVideoXCausalConv3d(\n",
       "            (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): CogVideoXUpBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-3): 4 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): CogVideoXUpsample3D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): CogVideoXUpBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (conv_shortcut): CogVideoXSafeConv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (1-3): 3 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): CogVideoXUpsample3D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): CogVideoXUpBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-3): 4 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): CogVideoXUpsample3D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): CogVideoXUpBlock3D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (conv_shortcut): CogVideoXSafeConv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "          (1-3): 3 x CogVideoXResnetBlock3D(\n",
       "            (nonlinearity): SiLU()\n",
       "            (norm1): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (norm2): CogVideoXSpatialNorm3D(\n",
       "              (norm_layer): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv_y): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "              (conv_b): CogVideoXCausalConv3d(\n",
       "                (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "              )\n",
       "            )\n",
       "            (conv1): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): CogVideoXCausalConv3d(\n",
       "              (conv): CogVideoXSafeConv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): CogVideoXSpatialNorm3D(\n",
       "      (norm_layer): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_y): CogVideoXCausalConv3d(\n",
       "        (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (conv_b): CogVideoXCausalConv3d(\n",
       "        (conv): CogVideoXSafeConv3d(16, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): CogVideoXCausalConv3d(\n",
       "      (conv): CogVideoXSafeConv3d(128, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ref = 1\n",
    "num_target = 49\n",
    "height = model_config_yaml['height']\n",
    "width = model_config_yaml['width']\n",
    "\n",
    "transformer.requires_grad_(True)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "if accelerator.state.deepspeed_plugin:\n",
    "    ds_cfg = accelerator.state.deepspeed_plugin.deepspeed_config\n",
    "    if \"fp16\" in ds_cfg and ds_cfg[\"fp16\"][\"enabled\"]:\n",
    "        weight_dtype = torch.float16\n",
    "    elif \"bf16\" in ds_cfg and ds_cfg[\"bf16\"][\"enabled\"]:\n",
    "        weight_dtype = torch.float16\n",
    "    else:\n",
    "        weight_dtype = torch.float32\n",
    "else:\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    else:\n",
    "        weight_dtype = torch.float32\n",
    "\n",
    "def unwrap_model(m):\n",
    "        m = accelerator.unwrap_model(m)\n",
    "        return m._orig_mod if hasattr(m, \"_orig_mod\") else m\n",
    "\n",
    "transformer.to(accelerator.device)\n",
    "vae.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e5850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meternity304\u001b[0m (\u001b[33mthueval\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/horai.dgpsrv/ondemand28/harryscz/diffusion/wandb/run-20250601_163736-mqk7joqv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thueval/cogvideox/runs/mqk7joqv' target=\"_blank\">magic-field-14</a></strong> to <a href='https://wandb.ai/thueval/cogvideox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thueval/cogvideox' target=\"_blank\">https://wandb.ai/thueval/cogvideox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thueval/cogvideox/runs/mqk7joqv' target=\"_blank\">https://wandb.ai/thueval/cogvideox/runs/mqk7joqv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainable_parameters =  list(filter(lambda p: p.requires_grad,transformer.parameters()))\n",
    "params_to_optimize = [{\"params\": trainable_parameters, \"lr\": learning_rate}]\n",
    "use_deepspeed_optimizer = (\n",
    "    accelerator.state.deepspeed_plugin is not None\n",
    "    and \"optimizer\" in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    ")\n",
    "optimizer = get_optimizer(\n",
    "    learning_rate=learning_rate,\n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.95, \n",
    "    adam_epsilon=1e-8, \n",
    "    adam_weight_decay=1e-4, \n",
    "    params_to_optimize=params_to_optimize, \n",
    "    use_deepspeed=use_deepspeed_optimizer\n",
    ")\n",
    "\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "# Build lr_scheduler\n",
    "use_deepspeed_scheduler = (\n",
    "    accelerator.state.deepspeed_plugin is not None\n",
    "    and \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n",
    ")\n",
    "if use_deepspeed_scheduler:\n",
    "    # Let DeepSpeed handle scheduling\n",
    "    lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,  # handled by deepspeed\n",
    "        num_warmup_steps=lr_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "        num_cycles=lr_num_cycles,\n",
    "        power=lr_power,\n",
    "    )\n",
    "else:\n",
    "    # Normal HF scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,  # placeholder, will be replaced after prepare()\n",
    "        num_warmup_steps=lr_warmup_steps,\n",
    "        num_training_steps=max_train_steps,\n",
    "        num_cycles=lr_num_cycles,\n",
    "        power=lr_power,\n",
    "    )\n",
    "\n",
    "transformer, vae, optimizer, lr_scheduler, train_dataloader = accelerator.prepare(transformer, vae, optimizer, lr_scheduler, train_dataloader)\n",
    "if accelerator.is_main_process:\n",
    "    tracker_name = tracker_name or \"cogvideox-diffusion\"\n",
    "    accelerator.init_trackers(tracker_name, config={\"dropout\": 0.0, \"learning_rate\": learning_rate})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59aa8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/01/2025 16:37:39 - INFO - __main__ - ***** Running training *****\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Num trainable parameters    = 1691828832\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Num examples                = 10\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Num batches each epoch      = 10\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Num epochs                  = 1\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Batch size per device       = 1\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Total train batch size      = 2\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Gradient accumulation steps = 2\n",
      "06/01/2025 16:37:39 - INFO - __main__ -   Total optimization steps    = 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010dd637a03140c5b54d4cc02313a81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_batch_size = batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "num_trainable_parameters = sum(p.numel() for p in trainable_parameters)\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num trainable parameters    = {num_trainable_parameters}\")\n",
    "logger.info(f\"  Num examples                = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num batches each epoch      = {len(train_dataloader)}\")\n",
    "logger.info(f\"  Num epochs                  = {num_train_epochs}\")\n",
    "logger.info(f\"  Batch size per device       = {batch_size}\")\n",
    "logger.info(f\"  Total train batch size      = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient accumulation steps = {gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps    = {max_train_steps}\")\n",
    "\n",
    "first_epoch = 0\n",
    "initial_global_step = 0\n",
    "global_step = 0\n",
    "progress_bar = tqdm(\n",
    "    range(0, max_train_steps),\n",
    "    desc=\"Steps\",\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "vae_scale_factor_spatial = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "torch.cuda.empty_cache()\n",
    "model_conf = transformer.module.config if hasattr(transformer, \"module\") else transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1038387/4138580761.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch, num_train_epochs):\n",
    "    transformer.train()\n",
    "\n",
    "    def encode_video(video):\n",
    "        # video is originally CPU float32 or uint8 → move to GPU float32\n",
    "        video = video.to(accelerator.device)  # default dtype remains float32\n",
    "        with torch.no_grad():\n",
    "            # Under AMP autocast, this conv runs in bf16 for speed, then returns a float32 output.\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                latent_dist = vae.encode(video).latent_dist.sample() * vae.config.scaling_factor\n",
    "        # latent_dist is returned as float32 (AMP always returns float32 for downstream use)\n",
    "        return latent_dist.contiguous()\n",
    "\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        models_to_accumulate =  [transformer]\n",
    "        with accelerator.accumulate(models_to_accumulate): \n",
    "            latent_chunks = []\n",
    "            permuted_latents = []\n",
    "            ref_mask_chunks = []\n",
    "            cond_mask_chunks = []\n",
    "\n",
    "            for chunk_id in range(len(batch[\"video_chunks\"])):\n",
    "                raw_video = batch[\"video_chunks\"][chunk_id]\n",
    "\n",
    "                # 1. Encode to latent (→ [B, C_z, F_lat, h_lat, w_lat]):\n",
    "                pre = encode_video(raw_video)\n",
    "                if is_uncond:\n",
    "                    pre = pre * 0.0\n",
    "                latent_chunks.append(pre)\n",
    "                permuted_latents.append(pre.permute(0,2,1,3,4))\n",
    "\n",
    "                # 2. downsample reference mask to latent resolution\n",
    "                raw_ref_mask = batch[\"cond_chunks\"][\"ref_mask\"][chunk_id].to(\n",
    "                    dtype=weight_dtype, device=accelerator.device\n",
    "                )\n",
    "                B, C_z, F_lat, h_lat, w_lat = pre.shape\n",
    "                mask_down = torch.nn.functional.interpolate(\n",
    "                    raw_ref_mask.unsqueeze(1), size=(F_lat, h_lat, w_lat), mode=\"nearest\"\n",
    "                )\n",
    "                if is_uncond:\n",
    "                    mask_down = mask_down * 0.0\n",
    "                ref_mask_chunks.append(mask_down)\n",
    "                mask_cond = mask_down.permute(0, 2, 1, 3, 4)\n",
    "                cond_mask_chunks.append(mask_cond)\n",
    "            # 3. build cond chunks of length 1 from ref_mask_chunks\n",
    "            cond_chunks = [ rm for rm in cond_mask_chunks ]\n",
    "\n",
    "            # Sample and forward noise\n",
    "            B, C_z, F, h_z, w_z = latent_chunks[0].shape\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                scheduler.config.num_train_timesteps,\n",
    "                (B,),\n",
    "                device=accelerator.device,\n",
    "            ).long()\n",
    "\n",
    "            noisy_preperm_list = []\n",
    "            for chunk_id, clean_latent in enumerate(latent_chunks):\n",
    "                # clean_latent = [B, C_z, F_lat, h, w]\n",
    "                noise = torch.randn_like(clean_latent)\n",
    "                noisy = scheduler.add_noise(clean_latent, noise, timesteps)\n",
    "                noisy = noisy.to(dtype=weight_dtype)\n",
    "                rm = ref_mask_chunks[chunk_id]  # [B, 1, F_lat, h, w]\n",
    "                one_bf16 = torch.ones_like(rm)\n",
    "                merged = noisy * (one_bf16 - rm) + clean_latent * rm\n",
    "                merged = merged.to(dtype=weight_dtype)  \n",
    "                noisy_preperm_list.append(merged)\n",
    "\n",
    "            noisy_latents = [m.permute(0, 2, 1, 3, 4) for m in noisy_preperm_list]\n",
    "\n",
    "            # Get Sequence info\n",
    "            sequence_infos = []\n",
    "            for chunk_id, clean_latent in enumerate(latent_chunks):\n",
    "                F_lat = clean_latent.shape[2]\n",
    "                seq_idx = torch.arange(0, F_lat, device=accelerator.device)\n",
    "                sequence_infos.append((False, seq_idx))\n",
    "\n",
    "            inner_dim = transformer.config.num_attention_heads * transformer.config.attention_head_dim  # 30×64=1920\n",
    "            fake_text_embeds = torch.zeros((B, 1, inner_dim), dtype=weight_dtype, device=accelerator.device)\n",
    "\n",
    "            # 5b) Fake audio embeddings:\n",
    "            audio_feature_dim  = 768   # _must_ match what Wav2Vec would have produced\n",
    "            fake_audio_embeds  = torch.zeros(\n",
    "                (B, F_lat, audio_feature_dim),\n",
    "                device=accelerator.device,\n",
    "                dtype=weight_dtype\n",
    "            )\n",
    "            \n",
    "            model_output = transformer(\n",
    "                hidden_states=noisy_latents,\n",
    "                condition=cond_chunks,\n",
    "                sequence_infos=sequence_infos,\n",
    "                timestep=timesteps,\n",
    "                audio_embeds=fake_audio_embeds,\n",
    "                encoder_hidden_states=fake_text_embeds,\n",
    "                image_rotary_emb=None,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # compute loss on non-reference pixels\n",
    "            ref_mask = torch.cat(ref_mask_chunks, dim=2)\n",
    "            non_ref_mask = 1 - ref_mask\n",
    "            non_ref_mask = non_ref_mask.permute(0, 2, 1, 3, 4)  \n",
    "\n",
    "            model_output = torch.cat(model_output, dim=1)\n",
    "            model_input = torch.cat(permuted_latents, dim=1)\n",
    "            noisy_input = torch.cat(noisy_latents, dim=1)\n",
    "\n",
    "            model_pred = scheduler.get_velocity(model_output, noisy_input, timesteps)\n",
    "            alphas_cumprod = scheduler.alphas_cumprod[timesteps]\n",
    "            weights = 1.0 / (1.0 - alphas_cumprod)\n",
    "            while len(weights.shape) < len(model_pred.shape):\n",
    "                        weights = weights.unsqueeze(-1)\n",
    "\n",
    "            target = torch.cat(permuted_latents, dim=1)\n",
    "            loss = weights * ((model_pred - target) ** 2)\n",
    "            loss = loss * non_ref_mask / non_ref_mask.mean()\n",
    "            loss = torch.mean(loss.reshape(B, -1), dim=1).mean()\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr_scheduler.step(global_step)\n",
    "            global_step += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # Save checkpoint every args.checkpointing_steps\n",
    "            if accelerator.is_main_process and (global_step % checkpointing_steps == 0):\n",
    "                save_path = os.path.join(output_dir, f\"checkpoint-{global_step}.pt\")\n",
    "                save_dict = {\n",
    "                    \"state_dict\": unwrap_model(transformer).state_dict(),\n",
    "                    \"optimizer\":  optimizer.state_dict(),\n",
    "                    \"global_step\": global_step,\n",
    "                    \"epoch\":      epoch,\n",
    "                }\n",
    "                torch.save(save_dict, save_path)\n",
    "                logger.info(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "            accelerator.log({\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}, step=global_step)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab25d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
