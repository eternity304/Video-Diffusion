{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38721913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "import math\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import imageio\n",
    "from train.trainUtils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6492c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(arg_list=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Unconditioned Video Diffusion Inference\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset-path\", type=str, required=True,\n",
    "        help=\"Directory containing input reference videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained-model-name-or-path\", type=str, required=True,\n",
    "        help=\"Path or HF ID where transformer/vae/scheduler are stored.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-path\", type=str, required=True,\n",
    "        help=\"Path to fine‐tuned checkpoint containing transformer state_dict.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\", type=str, required=True,\n",
    "        help=\"Where to write generated videos.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model-config\", type=str, required=True,\n",
    "        help=\"YAML file describing model params (height, width, num_reference, num_target, etc.)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=1,\n",
    "        help=\"Batch size per device (usually 1 for inference).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-inference-steps\", type=int, default=50,\n",
    "        help=\"Number of reverse diffusion steps to run.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed-precision\", type=str, default=\"bf16\",\n",
    "        help=\"Whether to run backbone in 'fp16', 'bf16', or 'fp32'.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42,\n",
    "        help=\"Random seed for reproducibility.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--shuffle\", type=int, default=False,\n",
    "        help=\"Whether to shuffle dataset. Usually False for inference.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--is-uncond\", type=bool, default=False,\n",
    "        help=\"\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning-rate\", type=float, default=1e-4\n",
    "    )\n",
    "\n",
    "    # If arg_list is None, argparse picks up sys.argv; \n",
    "    # otherwise it treats arg_list as the full argv list.\n",
    "    return parser.parse_args(arg_list)\n",
    "\n",
    "args = [\n",
    "    \"--dataset-path\", \"/scratch/ondemand28/harryscz/head_audio/data/data256/uv\",\n",
    "    \"--pretrained-model-name-or-path\", \"/scratch/ondemand28/harryscz/model/CogVideoX-2b\",\n",
    "    \"--checkpoint-path\",  \"/scratch/ondemand28/harryscz/head_audio/trainOutput/checkpoint-1000.pt\",\n",
    "    \"--output-dir\",  \"/scratch/ondemand28/harryscz/diffusion/videoOut\",\n",
    "    \"--model-config\",  \"/scratch/ondemand28/harryscz/diffusion/train/model_config.yaml\",\n",
    "    \"--batch-size\",  \"1\",\n",
    "    \"--num-inference-steps\",  \"50\",\n",
    "    \"--mixed-precision\",  \"no\",\n",
    "    \"--seed\",  \"42\",\n",
    "    \"--shuffle\",  \"0\",\n",
    "    \"--learning-rate\", \"0.0001\"\n",
    "]\n",
    "\n",
    "args = parse_args(args)\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7bee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1823700/782281243.py\", line 32, in <module>\n",
      "    logger.info(\"Accelerator state:\", accelerator.state)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/logging/__init__.py\", line 1806, in info\n",
      "    self.log(INFO, msg, *args, **kwargs)\n",
      "  File \"/scratch/ondemand28/harryscz/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/accelerate/logging.py\", line 63, in log\n",
      "    self.logger.log(level, msg, *args, **kwargs)\n",
      "Message: 'Accelerator state:'\n",
      "Arguments: (Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      ",)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "with open(args.model_config, \"r\") as f: model_config = yaml.safe_load(f)\n",
    "if args.mixed_precision.lower() == \"fp16\":\n",
    "    dtype = torch.float16\n",
    "elif args.mixed_precision.lower() == \"bf16\":\n",
    "    dtype = torch.bfloat16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir,\n",
    "                                                    logging_dir=os.path.join(args.output_dir, \"logs\"))\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)\n",
    "accelerator = Accelerator(mixed_precision=args.mixed_precision,\n",
    "                            project_config=accelerator_project_config,\n",
    "                            kwargs_handlers=[ddp_kwargs])\n",
    "\n",
    "# 2.4 Set random seed\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed + accelerator.process_index)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(\"Accelerator state:\", accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b50979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/13/2025 12:33:50 - INFO - __main__ - Number of test examples: 1\n"
     ]
    }
   ],
   "source": [
    "#### Dataset #####\n",
    "# Video data have shape [B, C, F, H, W]\n",
    "\n",
    "from data.VideoDataset import VideoDataset \n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "dataset = VideoDataset(\n",
    "    videos_dir=args.dataset_path,\n",
    "    num_ref_frames=1,\n",
    "    num_target_frames=49\n",
    ")\n",
    "if args.shuffle:\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=accelerator.num_processes,\n",
    "        rank=accelerator.process_index,\n",
    "        shuffle=True\n",
    "    )\n",
    "else:\n",
    "    sampler = None\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    # sampler=sampler,\n",
    "    collate_fn=lambda x: x[0],   # since dataset returns already‐batched items\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "logger.info(f\"Number of test examples: {len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96b9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b were not used when initializing CAPVideoXTransformer3DModel: \n",
      " ['patch_embed.text_proj.weight, patch_embed.text_proj.bias']\n",
      "Some weights of CAPVideoXTransformer3DModel were not initialized from the model checkpoint at /scratch/ondemand28/harryscz/model/CogVideoX-2b and are newly initialized: ['patch_embed.cond_proj.weight', 'patch_embed.ref_temp_proj.bias', 'patch_embed.audio_proj.bias', 'patch_embed.ref_temp_proj.weight', 'patch_embed.cond_proj.bias', 'patch_embed.audio_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#### Load Model ####\n",
    "device = \"cuda\"\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXDDIMScheduler\n",
    "from model.cap_transformer import CAPVideoXTransformer3DModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from inference.inference_pipeline import *\n",
    "\n",
    "transformer = CAPVideoXTransformer3DModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    device_map=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float32,\n",
    "    cond_in_channels=1,  # only one channel (the ref_mask)\n",
    "    sample_width=model_config[\"width\"] // 8,\n",
    "    sample_height=model_config[\"height\"] // 8,\n",
    "    max_text_seq_length=1,\n",
    "    max_n_references=model_config[\"max_n_references\"],\n",
    "    apply_attention_scaling=model_config[\"use_growth_scaling\"],\n",
    "    use_rotary_positional_embeddings=False,\n",
    ")\n",
    "\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "scheduler = CogVideoXDDIMScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"scheduler\",\n",
    "    prediction_type=\"v_prediction\"\n",
    ")\n",
    "\n",
    "# if args.enable_slicing: vae.enable_slicing()\n",
    "# if args.enable_tiling:  vae.enable_tiling()\n",
    "\n",
    "vae.eval().to(dtype)\n",
    "transformer.eval().to(dtype)\n",
    "\n",
    "vae, transformer, scheduler, data_loader = accelerator.prepare(vae, transformer, scheduler, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac21f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbba9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters =  list(filter(lambda p: p.requires_grad, transformer.parameters()))\n",
    "params_to_optimize = [{\"params\": trainable_parameters, \"lr\": args.learning_rate}]\n",
    "\n",
    "optimizer = get_optimizer(\n",
    "    learning_rate=args.learning_rate,\n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.95, \n",
    "    adam_epsilon=1e-8, \n",
    "    adam_weight_decay=1e-4, \n",
    "    params_to_optimize=params_to_optimize, \n",
    "    # use_deepspeed=use_deepspeed_optimizer\n",
    ")\n",
    "\n",
    "def encode_video(vae, video):\n",
    "    with torch.no_grad():\n",
    "        dist = vae.encode(video).latent_dist.sample()\n",
    "    latent = dist * vae.config.scaling_factor\n",
    "    return latent.permute(0,2,1,3,4).contiguous()\n",
    "\n",
    "def unwrap_model(m):\n",
    "    m = accelerator.unwrap_model(m)\n",
    "    return m._orig_mod if hasattr(m, \"_orig_mod\") else m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype=torch.float32\n",
    "transformer.train()\n",
    "\n",
    "for step, batch in enumerate(data_loader):\n",
    "    # models_to_accumulate =  [transformer] !!!!!!!!!!!!!!\n",
    "    # with accelerator.accumulate(models_to_accumulate): !!!!!!!!!!!!!!!\n",
    "        latent_chunks = []\n",
    "        ref_mask_chunks = []\n",
    "\n",
    "        # Initialize necessary data for diffusion\n",
    "        for i, video in enumerate(batch[\"video_chunks\"]):\n",
    "            video = video.to(accelerator.device).to(weight_dtype)\n",
    "\n",
    "            # Encode Video\n",
    "            latent = encode_video(vae, video) # [B, F, C_z, H, W]\n",
    "            latent_chunks.append(latent)\n",
    "\n",
    "            # Ref Mask Chunk, Mask of shape [B, F, H, W, C]\n",
    "            B, F_z, C, H, W = latent.shape\n",
    "            rm = torch.zeros((B, F_z, 1, H, W), device=accelerator.device, dtype=weight_dtype)\n",
    "            rm[:, 0] = 1.0\n",
    "            ref_mask_chunks.append(rm)\n",
    "\n",
    "        sequence_infos = [[False, torch.arange(chunk.shape[1])]for chunk in latent_chunks]\n",
    "        \n",
    "        # Sample Random Noise\n",
    "        B, F_z, C_z, H_z, W_z = latent_chunks[0].shape\n",
    "        timesteps = torch.randint(\n",
    "            1,\n",
    "            scheduler.config.num_train_timesteps,\n",
    "            (B,),\n",
    "            device=accelerator.device\n",
    "        ).long()\n",
    "\n",
    "        noised_latents = []\n",
    "        for idx, latent in enumerate(latent_chunks):\n",
    "            noise = torch.randn_like(latent, device=accelerator.device, dtype=weight_dtype)\n",
    "            noisy_latent = scheduler.add_noise(latent, noise, timesteps)\n",
    "            noised_latents.append(noisy_latent)\n",
    "\n",
    "        # Trivial Audio, Text, and Condition\n",
    "        audio_embeds = torch.zeros((B, F_z, 768), dtype=weight_dtype, device=accelerator.device)\n",
    "        text_embeds  = torch.zeros((B, 1,\n",
    "            unwrap_model(transformer).config.attention_head_dim * unwrap_model(transformer).config.num_attention_heads\n",
    "        ), dtype=weight_dtype, device=accelerator.device)\n",
    "        B, F_z, C_z, H_z, W_z = noised_latents[0].shape\n",
    "        zero_cond = [torch.zeros((B, F_z, 1, H_z, W_z), dtype=weight_dtype, device=accelerator.device)] * len(noised_latents)\n",
    "\n",
    "        # Predict Noise\n",
    "        model_outputs = transformer(\n",
    "            hidden_states=noised_latents,\n",
    "            encoder_hidden_states=text_embeds,\n",
    "            audio_embeds=audio_embeds,\n",
    "            condition=zero_cond,\n",
    "            timestep=timesteps,\n",
    "            sequence_infos=sequence_infos,\n",
    "            image_rotary_emb=None,\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "\n",
    "        # ref_mask = torch.cat(ref_mask_chunks, dim=1)\n",
    "        # non_ref_mask = 1. - ref_mask\n",
    "\n",
    "        model_output = torch.cat(model_outputs, dim=1)\n",
    "        model_input = torch.cat(latent_chunks, dim=1)\n",
    "        noisy_input = torch.cat(noised_latents, dim=1)\n",
    "\n",
    "        # print(\"model_output\", model_output.min(), model_output.max())\n",
    "        model_pred = scheduler.get_velocity(model_output, noisy_input, timesteps)\n",
    "\n",
    "        alpha_bar = scheduler.alphas_cumprod[timesteps].to(weight_dtype)\n",
    "        sigma_bar = (1 - alpha_bar).sqrt()\n",
    "        eps = (model_input - alpha_bar.sqrt() * noisy_input) / sigma_bar\n",
    "        v_true = alpha_bar.sqrt() * eps - sigma_bar * model_input\n",
    "        loss = F.mse_loss(model_pred, v_true)\n",
    "\n",
    "        accelerator.backward(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
